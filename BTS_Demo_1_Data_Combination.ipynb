{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c238848-8b10-4eef-8e42-aa8d475f9eff",
   "metadata": {},
   "source": [
    "### 1) Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "2f6e87fe-cf2f-48f0-9332-8a3e11f57c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import zipfile\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "id": "143e4459-2016-46b4-b100-d0ee49ef4307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully combined!           \n"
     ]
    }
   ],
   "source": [
    "def combine_zipped_data(root_data_folder_path):\n",
    "    '''\n",
    "    This function opens all zip files in a given folder and combines any csv data found\n",
    "    within them into a single Pandas DataFrame.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    root_data_folder_path: A string containing the path to a folder containing zip files\n",
    "                           with csv data.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A Pandas Dataframe with all csv data found in the given folder, combined together along\n",
    "    the index (0) axis.\n",
    "    '''\n",
    "    # Creating empty list of DataFrames\n",
    "    data_to_combine = []\n",
    "    # Looping through raw data folder\n",
    "    with os.scandir(root_data_folder_path) as root_data_folder:\n",
    "        total_files = len(os.listdir(root_data_folder_path))\n",
    "        current_progress = 0\n",
    "        for entry in root_data_folder:\n",
    "            # Displaying current progress\n",
    "            current_progress += 1\n",
    "            print(f\"Processing file {current_progress}/{total_files} ...\", end=\"\\r\")\n",
    "            # Searching for zipped data\n",
    "            if entry.name.endswith(\".zip\") and entry.is_file():\n",
    "                # Opening zipped data folders\n",
    "                with zipfile.ZipFile(root_data_folder_path + '/' + entry.name, \"r\") as zipped:\n",
    "                    for name in zipped.namelist():\n",
    "                        # Searching for csv files in zipped folders\n",
    "                        if name.endswith('.csv'):\n",
    "                            with zipped.open(name) as delay_data:\n",
    "                                # Reading csv and adding to list of datasets\n",
    "                                data_to_combine.append(pd.read_csv(delay_data, low_memory=False))\n",
    "    # Attempting to combine and return collected data\n",
    "    print('All files unpacked. Combining data...', end='\\r')\n",
    "    combined_data = pd.concat(data_to_combine)\n",
    "    print('Data successfully combined!           ')\n",
    "    return combined_data\n",
    "\n",
    "flight_df = combine_zipped_data(\"BTS_Data\")\n",
    "flight_df = flight_df.dropna(subset=[\"FlightDate\",\"DepTime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "id": "c1d41e17-e237-462e-8e17-c3c1f301c9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing all columns that are more than 5% NaN values \n",
    "flight_df = flight_df[flight_df.columns[flight_df.isna().sum() < flight_df.shape[0] / 20]]\n",
    "# Dropping rows of data with NaN in the target delay column (ArrDel15)\n",
    "flight_df = flight_df.dropna(subset=\"ArrDel15\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "id": "e7dcf046-bf2f-4f49-a8b9-753f7243d635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully combined!           \n"
     ]
    }
   ],
   "source": [
    "def combine_weather_data(root_data_folder_path):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    data_to_combine = []\n",
    "    with os.scandir(root_data_folder_path) as root_data_folder:\n",
    "        total_files = len(os.listdir(root_data_folder_path))\n",
    "        current_progress = 0\n",
    "        for entry in root_data_folder:\n",
    "            # Displaying current progress\n",
    "            current_progress += 1\n",
    "            print(f\"Processing file {current_progress}/{total_files} ...\", end=\"\\r\")\n",
    "            # Searching for csv data\n",
    "            if entry.is_file() and entry.name[-4:] == \".csv\":\n",
    "                # Collecting csv files for combination\n",
    "                airport_df = pd.read_csv(entry.path)\n",
    "                airport_df.loc[:,\"airport_code\"] = entry.name[:-4]\n",
    "                airport_df = airport_df.fillna(value={\"gust\":0})\n",
    "                data_to_combine.append(airport_df)\n",
    "    # Attempting to combine and return collected data\n",
    "    print('All files unpacked. Combining data...', end='\\r')\n",
    "    combined_data = pd.concat(data_to_combine)\n",
    "    print('Data successfully combined!           ')\n",
    "    return combined_data\n",
    "\n",
    "root_data_folder_path = \"WeatherData_Clean\"\n",
    "weather_df = combine_weather_data(root_data_folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "6f683b4c-83bb-4b95-9c8f-2d9021360b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping extremely small number of rows with null windspeed/dewpoint/temp values\n",
    "weather_df = weather_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "525a5b5f-bdf9-4796-bec0-087686fde64a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully attached!                                       \r"
     ]
    }
   ],
   "source": [
    "def match_flight_and_weather_data(flight_df, weather_df):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    # Preparing flight dataframe to recieve weather data rows\n",
    "    flight_df = flight_df.reset_index()\n",
    "    flight_df.loc[:,weather_df.columns] = pd.NA\n",
    "    weather_column_names = weather_df.columns\n",
    "    # Recording airport counts to display function's current progress\n",
    "    num_airports_to_inspect = len(flight_df.Origin.unique())\n",
    "    airports_with_known_weather_data = weather_df.airport_code.unique()\n",
    "    for ind, airport_code in enumerate(list(flight_df.Origin.unique())):\n",
    "        if airport_code in airports_with_known_weather_data:\n",
    "            print(f'Currently processing airport code {airport_code}. {ind}/{num_airports_to_inspect}  ', end='\\r')\n",
    "            # Obtaining flight data for current airport and sorting by departure time\n",
    "            airport_flight_df = flight_df[flight_df.Origin == airport_code]\n",
    "            airport_flight_df.loc[:,\"FlightDate\"] = (airport_flight_df.FlightDate + \" \" + airport_flight_df.DepTime.apply(int).apply(str).str.zfill(4).apply(lambda time: time if time != \"2400\" else \"2359\"))\n",
    "            airport_flight_df.loc[:,\"FlightDate\"] = airport_flight_df.FlightDate.apply(lambda datestring: datetime.datetime.strptime(datestring, \"%Y-%m-%d %H%M\"))\n",
    "            airport_flight_df = airport_flight_df.sort_values(by=\"FlightDate\")\n",
    "            # Obtaining weather data for current airport and sorting by measurement time\n",
    "            airport_weather_df = weather_df[weather_df.airport_code == airport_code]\n",
    "            airport_weather_df.loc[:,\"record_start_date\"] = airport_weather_df.record_start_date.apply(lambda datestring: datetime.datetime.strptime(datestring, \"%Y-%m-%d %H:%M:%S\"))\n",
    "            airport_weather_df = airport_weather_df.sort_values(by=\"record_start_date\")\n",
    "            # Matching flight and weather data by ascending along both date columns\n",
    "            current_flight_entry = 0\n",
    "            current_weather_entry = 0\n",
    "            while current_flight_entry < airport_flight_df.shape[0]:\n",
    "                while airport_flight_df.FlightDate.iloc[current_flight_entry] > airport_weather_df.record_start_date.iloc[current_weather_entry]:\n",
    "                    current_weather_entry += 1\n",
    "                flight_df.loc[airport_flight_df.index[current_flight_entry],weather_column_names] = airport_weather_df.iloc[current_weather_entry,:]\n",
    "                current_flight_entry += 1\n",
    "    print(\"Data successfully attached!                                       \", end=\"\\r\")\n",
    "    return flight_df\n",
    "\n",
    "# Restricting attachment to only airports with known weather data\n",
    "relevant_flights_df = flight_df[flight_df.Origin.isin(weather_df.airport_code.unique())]\n",
    "# Subsetting dataset for demo testing purposes (NOT FINAL)\n",
    "relevant_flights_df = relevant_flights_df[relevant_flights_df.Origin == \"SEA\"]\n",
    "relevant_flights_df = relevant_flights_df[relevant_flights_df.Year == 2022]\n",
    "relevant_flights_df = relevant_flights_df[relevant_flights_df.Month.isin((5,6,7))]\n",
    "demo_df = match_flight_and_weather_data(relevant_flights_df, weather_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "fd5b1a55-5f12-43f1-af71-7c6ce246e980",
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_df.to_pickle(\"combined_flight_data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
